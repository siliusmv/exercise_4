---
title: "TDT4173 -- Assignment 4"
site: bookdown::bookdown_site
author: 
- "Silius Mortens√∏nn Vandeskog (siliusmv)"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  bookdown::pdf_document2:
    toc: no
    fig_caption: yes
    number_sections: yes
---

```{r setup, include=FALSE, cache = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, error = FALSE, out.width = "70%", fig.align = "center")
source("code.R")
knitr::read_chunk("code.R")
library(reticulate)

```


# Theory

## 1)

The core idea of deep learning is...

## 2)


## 3)



# Programming

```{r}
# Extract data
knn_class <- read.csv("dataset/dataset/knn_classification.csv")
knn_reg <- read.csv("dataset/dataset/knn_regression.csv")
ada_train <- read.csv("dataset/dataset/adaboost_train.csv")
ada_test <- read.csv("dataset/dataset/adaboost_test.csv")
```


## 2.1)

We implement a k-NN algorithm from scratch. Then the program is reused in order to implement a k-NN regression and classification. The code can be seen below

```{r knn}
```

We now use the algorithms with $k = 10$ for the $124^\text{th}$ example of the given data sets. As seen below, the algorithm predicts a value of $1.6$ for the regression and $2$ for the classification.


```{r}

knn(k = 10,
    data = knn_reg,
    point = as.vector(as.matrix((knn_reg[124, 1:3]))),
    type = "reg")


knn(k = 10,
    data = knn_class,
    point = as.vector(as.matrix((knn_class[124, 1:4]))),
    type = "class")


```

```{python, results = "hide", echo = TRUE}
# -*- coding: utf-8 -*-
import sys
import numpy as np
import os
import sklearn
import matplotlib.pyplot as plt


cwd = os.getcwd()
sys.path.insert(0, cwd)
import stupidname as ss


knn_class = np.genfromtxt("dataset/dataset/knn_classification.csv", 
delimiter=",", skip_header=1)
knn_reg = np.genfromtxt("dataset/dataset/knn_regression.csv", 
delimiter=",", skip_header=1)
ada_test = np.genfromtxt("dataset/dataset/adaboost_test.csv", 
delimiter=",", skip_header=1)
ada_train = np.genfromtxt("dataset/dataset/adaboost_train.csv", 
delimiter=",", skip_header=1)


```



```{python}
err = ss.testAdaBoost(15, ada_train, ada_test)

for e in err:
  print("%.2f" % e)




```



```{python}

X_train, X_test, y_train, y_test = ss.getData()


from sklearn.neighbors import KNeighborsClassifier as KNN


neigh = KNN()
neigh.fit(X_train, y_train)

pred_neigh = neigh.predict(X_test)




from sklearn import svm

clf = svm.SVC()
clf.fit(X_train, y_train)

pred_svm = clf.predict(X_test)



from sklearn.ensemble import RandomForestClassifier as RFC

forest = RFC()
forest.fit(X_train, y_train)

pred_forest = forest.predict(X_test)





from sklearn.metrics import confusion_matrix
from mlxtend.plotting import plot_confusion_matrix

cnf_neigh = confusion_matrix(y_test, pred_neigh)
cnf_svm = confusion_matrix(y_test, pred_svm)
cnf_forest = confusion_matrix(y_test, pred_forest)


fig_neigh, ax = plot_confusion_matrix(conf_mat = cnf_neigh)
plt.show()
fig_svm, ax = plot_confusion_matrix(conf_mat = cnf_svm)
plt.show()
fig_forest, ax = plot_confusion_matrix(conf_mat = cnf_forest)
plt.show()

```

