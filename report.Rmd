---
title: "TDT4173 -- Assignment 4"
site: bookdown::bookdown_site
author: 
- "Silius Mortens√∏nn Vandeskog (siliusmv)"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  bookdown::pdf_document2:
    toc: no
    fig_caption: yes
    number_sections: no
---

```{r setup, include=FALSE, cache = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, error = FALSE, out.width = "70%", fig.align = "center", cache = TRUE)
source("code.R")
knitr::read_chunk("code.R")
library(reticulate)

```






# Theory

## 1)

Deep learning is a class of machine learning algorithms that allows for computational models composed of multiple processing layers able to learn representations of data with multiple levels of abstraction. E.g., a deep learner like artificial neural networks attempts to better solve their problems by adding depth layers in the neural network, an approach which can be compared to sifting data with multiple sifters in order to distil finer and finer pieces of data.

Deep learning algorithms seek to discover and develop complex structures for large high-dimensional data sets, cf. problems of image recognition and language understanding and translation, by applying a backpropagation algorithm which alters the internal parameters used in computing the representations in each of the deep network's layer from the representation in the previous layer. I.e., each successive layer uses the output from the previous layer as input, and the backpropagation algorithm seeks to minimise squared errors between network output values and the corresponding target values.

Hence, the deep learning's backpropagation (or similar optimisation) algorithm attempts to learn informative representations of the data with little to no a priori knowledge. A deep learning image recognition scheme might e.g. learn to distinguish between images of "cats" and "no cats", by looking at manually labelled images without any a priori knowledge, and evolving their own set of relevant characteristics from the learning material. In contrast, "shallow" learning is often based upon the developer having some a priori knowledge of which specific features of the input data that are more important in determining the correct answer. Examples of shallow learners are decision trees, SVM and Naive Bayes. The emphasis in shallow learning is thus often on feature engineering and selection, while deep learners usually emphasise defining the most useful computational graph topology and optimising parameters correctly.

## 2)

k-NN is a non-parametric method used for classification and regression, where the input consists of the $k$ closest training examples in the feature space. In classification problems a vote is taken amongst its $k$ closest neighbours, and in regression problems the output is some statistic based upon the the values of the neighbours. k-NN is consequently a lazy learner, where the function is only approximated locally - thus allowing for successive changes in the system by adding the new instances to the feature space - and all computation is deferred until classification. Lazy learners are most useful for large data sets with few attributes, and disadvantages are slow evaluation time, sensitivity to local structures of the data and noise, and a large space requirement to store the entire training data set.

A deep learner is however an eager learner; unlike k-NN the training times might be very long. While k-NN, however, usually have a slow evaluation time, a deep learner applies the learned network to subsequent instances very fast. Furthermore, a deep learner is also quite robust to errors and noise in the training data.

The ability of the developer to understand the learned target function of the deep learner is not important, and the weights learned by neural networks are usually difficult to interpret. This starkly differs from decision trees, which are simple to understand and interpret. Decision trees are widely used for inductive inference, and the algorithm is capable of learning disjunctive expressions. They are appropriate for problems where instances are represented by attribute-value pairs, e.g. an instance ``temperature'' with the value ``hot''. The training data may have missing attribute data and the algorithm is quite robust to errors, but decision trees are unstable and often relatively inaccurate. Calculations can too get very complex when many values and attributes are uncertain, and when dealing with many closely linked outcomes.

SVM is a supervised learning algorithm that analyse data used for classification and regression analysis. Given a set of training data, each instance is marked as belonging to one of two categories. The SVM algorithm then constructs a hyperplane separating the two categories of data. (A good separation, which is desirable, is achieved by choosing the hyperplane having the largest distance to the nearest data points of either category; if the categories are not linearly separable, the data set can be mapped to a high-dimensional space in order to make the separation easier.) New instances can thus easily be marked belonging to any of the two categories. SVM is however sensitive to noise - a small number of mislabelled examples can dramatically decrease the performance of the algorithm. SVM furthermore only considers two categories.

## 3)

Ensemble methods are algorithms that combine several machine learning techniques into one predictive model. It is in a way a reflection of the ``no free lunch'' principle, which states that no single algorithm is the best at all times. Ensemble methods creates several classifiers from the training data, and then combines these into one classifier more accurate than any of its members. This is very effective if one has data and models with high variance, a lot of bias, and low stability. Then combining multiple independent and diverse learning techniques, each of which is at least more accurate than random guessing, random errors cancel each other out.

Bagging is an ensemble method designed to improve stability and accuracy of machine learning algorithms, further reducing variance and helping avoiding overfitting. Having a training data set, several bootstrap samples are drawn and a predictor for each data subset is trained. An ensemble predictor might then be created by e.g. voting or averaging the individual predictors.

Boosting, an ensemble method primarily used to reduce bias and variance, involves incrementally building an ensemble by training each new model instance to emphasise the training instances that, in the previous models, were misclassified. Hence, each new step creates a classifier only slightly correlated with the true classification, and in the end the classifiers are weighted, in a way that is related to its predictive accuracy of training data, and then combined. While showing a better accuracy than bagging, boosting have shown tendencies to overfit the training data.

The Bayes Optimal Classifier is an ensemble of all hypotheses in the hypothesis space. Each hypothesis is given a weight proportional to the likelihood of the sample if the given hypothesis were to be true, and next multiplied by the a priori probability of the hypothesis. While technically being a strong ensemble, it is however usually difficult to implement for other than the simplest of problems, due to most hypothesis spaces being too large to iterate over, as well as difficulty in estimating both the a priori probabilities of each hypothesis and the probability of a sample given a hypothesis to be true.

# Programming

```{r}
# Extract data
knn_class <- read.csv("dataset/dataset/knn_classification.csv")
knn_reg <- read.csv("dataset/dataset/knn_regression.csv")
ada_train <- read.csv("dataset/dataset/adaboost_train.csv")
ada_test <- read.csv("dataset/dataset/adaboost_test.csv")
```


## 2.1)

We implement a k-NN algorithm from scratch. Then the program is reused in order to implement a k-NN regression and classification. The code can be seen below

```{r knn}
```

We now use the algorithms with $k = 10$ for the $124^\text{th}$ example of the given data sets. As seen below, the algorithm predicts a value of $1.6$ for the regression and $2$ for the classification.


```{r}

knn(k = 10,
    data = knn_reg,
    point = as.vector(as.matrix((knn_reg[124, 1:3]))),
    type = "reg")


knn(k = 10,
    data = knn_class,
    point = as.vector(as.matrix((knn_class[124, 1:4]))),
    type = "class")


```


## 2.2)

We implement tha AdaBoost algorithm from scratch. The code can be seen in the print-out below. We use the `DecisionTreeClassifier` from `sklearn` with a miximum depth of 1 as our weak learner.

```{python}

# Returns classifier for some data
# using T iterations of adaBoost
def adaBoost(data, T):
    dim = data.shape
    
    x = data[0:dim[0], 2:(dim[1]-1)]     
    y = data[0:dim[0], 1]

    classifiers = []
    all_as = []
    
    weight = np.ones(dim[0]) * (1. / dim[0])

    for i in range(T):
            
        clf = tree.DecisionTreeClassifier(max_depth = 1)
        clf = clf.fit(x, y, sample_weight = weight)
        pred = clf.predict(x)
        
        epsilon = sum((pred != y) * weight)
        a = (1. / 2) * np.log((1 - epsilon) / epsilon)
        
        nw = weight * np.e**(-a * y * pred)
        nw = nw / sum(nw)
        weight = nw
        
        classifiers.append(clf)
        all_as.append(a)
        
    return(list([classifiers, all_as]))
        
# Returns predictions from an
# adaBoost algorithm
def adaPred(points, classifier):
    
    n = len(classifier[0])
    
    s = np.zeros(points.shape[0])
        
    for i in range(n):
        s = s + classifier[0][i].predict(points) * classifier[1][i]
    
    return(np.sign(s))
    
# Misclassification error    
def misClass(pred, y):
    return(sum(pred != y) / float(len(y)))


# Return an error vector for iteration 1 through max_iter
def testAdaBoost(max_iter, train_data, test_data):
    
    test_dim = test_data.shape
    x_test = test_data[0:test_dim[0], 2:(test_dim[1]-1)]
    y_test = test_data[0:test_dim[0], 1]
    errors = []
    
    for i in range(max_iter):
        classifier = adaBoost(train_data, i+1)
        pred = adaPred(x_test, classifier)
        errors.append(misClass(pred, y_test))
        
    return(errors)

```

Firstly, we load the data and modules needed,

```{python, results = "hide", echo = TRUE}
# -*- coding: utf-8 -*-
import sys
import numpy as np
import os
import sklearn
import matplotlib.pyplot as plt


cwd = os.getcwd()
sys.path.insert(0, cwd)
import ex_code as ss


knn_class = np.genfromtxt("dataset/dataset/knn_classification.csv", 
delimiter=",", skip_header=1)
knn_reg = np.genfromtxt("dataset/dataset/knn_regression.csv", 
delimiter=",", skip_header=1)
ada_test = np.genfromtxt("dataset/dataset/adaboost_test.csv", 
delimiter=",", skip_header=1)
ada_train = np.genfromtxt("dataset/dataset/adaboost_train.csv", 
delimiter=",", skip_header=1)


```


We want to test the AdaBoost algorithm for different number of iterations. Since the true target values are known we can calculate the misclassification error for AdaBoost algorithms with 1 to 15 iterations. The error rates are printed below.

```{python}
err = ss.testAdaBoost(15, ada_train, ada_test)

for e in err:
  print("%.2f" % e)




```

We see that the error rate steadily declines for one to 10 iterations of the algorithm. After this, the error rate starts oscillating. This might be an indicator of overfitting. A safe value for the number of iterations with this data seems to be around 10. After this we cannot trust the algorithm any more, as the error might be much larger. It could als be smaller, but it is hard to know.

## 2.3)

The code used for this task can be found below.

```{python}

# Loads data for task 2.3)
def getData():
    from sklearn.datasets import load_digits
    from sklearn.model_selection import train_test_split as TTS
    digits = load_digits()
    
    X_train, X_test, y_train, y_test = TTS(digits.data, digits.target, random_state=42)
    
    return(X_train, X_test, y_train, y_test)




X_train, X_test, y_train, y_test = ss.getData()


from sklearn.neighbors import KNeighborsClassifier as KNN


neigh = KNN()
neigh.fit(X_train, y_train)

pred_neigh = neigh.predict(X_test)




from sklearn import svm

clf = svm.SVC()
clf.fit(X_train, y_train)

pred_svm = clf.predict(X_test)



from sklearn.ensemble import RandomForestClassifier as RFC

forest = RFC()
forest.fit(X_train, y_train)

pred_forest = forest.predict(X_test)

```

We now want to test the different predictors. To do this we create and plot the confusion matrix for each prediciton. These can be seen below. The true label is found along the y-axis ,and the predicted label is found along the x-axis.

```{python}



from sklearn.metrics import confusion_matrix
from mlxtend.plotting import plot_confusion_matrix

cnf_neigh = confusion_matrix(y_test, pred_neigh)
cnf_svm = confusion_matrix(y_test, pred_svm)
cnf_forest = confusion_matrix(y_test, pred_forest)


fig_neigh, ax = plot_confusion_matrix(conf_mat = cnf_neigh)
plt.show()
fig_svm, ax = plot_confusion_matrix(conf_mat = cnf_svm)
plt.show()
fig_forest, ax = plot_confusion_matrix(conf_mat = cnf_forest)
plt.show()

```

We can see that both the kNN and random forest implementationw worked quite well. The kNN had a lot less errors than the random forest, though. We see that the SVM algorithm faultily classifies extremely many of the labels as 8, thus making it perform very poorly. There was not any similar patterns for the other algorithms. Apart from the predictions of the label 8, all other predictions from the SVM method are correct, though. This will of course change when we change the parameters in the three methods. It is very hard to compare the methods with unlimited possible parameter values, though. This is the reason why we useed the "standard" parameters defined in `sklearn` for all the methods.

